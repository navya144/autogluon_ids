{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the data files into a single df containing attack and benign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "Monday-WorkingHours.pcap_ISCX.csv\n",
      "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "Wednesday-workingHours.pcap_ISCX.csv\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from df_helpers import non_persistent_preprocessing\n",
    "\n",
    "np.random.seed(29)\n",
    "\n",
    "non_persistent_file_names = ['Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', \n",
    "           'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "           'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "           'Monday-WorkingHours.pcap_ISCX.csv',\n",
    "           'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "           'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "           'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "           'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in non_persistent_file_names:\n",
    "    print(file)\n",
    "    df_temp = TabularDataset('non-persistent/'+file)\n",
    "    df_temp.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "    df_temp.dropna(inplace=True)\n",
    "    df = pd.concat([df_temp, df])\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Remove duplicate column and whitespace from all columns\n",
    "df = non_persistent_preprocessing(df)\n",
    "    \n",
    "df.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop data for some uncommon threat types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_df = df.loc[df['Label'] == 'BENIGN']\n",
    "malicious_df = df.loc[df['Label'] != 'BENIGN'].copy(deep=True)\n",
    "\n",
    "malicious_df.drop(malicious_df[malicious_df['Label'] == 'Heartbleed'].index, inplace=True)\n",
    "malicious_df.drop(malicious_df[malicious_df['Label'] == 'Web Attack � Sql Injection'].index, inplace=True)\n",
    "malicious_df.drop(malicious_df[malicious_df['Label'] == 'Infiltration'].index, inplace=True)\n",
    "malicious_df.drop(malicious_df[malicious_df['Label'] == 'Web Attack � XSS'].index, inplace=True)\n",
    "malicious_df.drop(malicious_df[malicious_df['Label'] == 'Web Attack � Brute Force'].index, inplace=True)\n",
    "malicious_df.drop(malicious_df[malicious_df['Label'] == 'Bot'].index, inplace=True)\n",
    "\n",
    "merged_df = pd.concat([benign_df, malicious_df])\n",
    "merged_df = merged_df.sample(frac=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "BENIGN              2271320\n",
       "DoS Hulk             230124\n",
       "PortScan             158804\n",
       "DDoS                 128025\n",
       "DoS GoldenEye         10293\n",
       "FTP-Patator            7935\n",
       "SSH-Patator            5897\n",
       "DoS slowloris          5796\n",
       "DoS Slowhttptest       5499\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the whitespace and duplicate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # some column names have trailing white space!!!\n",
    "# column_names_without_space = []\n",
    "# for x in list(merged_df):\n",
    "#     column_names_without_space.append(x.strip())\n",
    "\n",
    "# column_name_updates = {}\n",
    "# for old_column_name, new_column_name in zip(list(merged_df), column_names_without_space):\n",
    "#     column_name_updates[old_column_name] = new_column_name\n",
    "\n",
    "# merged_df.rename(columns=column_name_updates, inplace=True)\n",
    "\n",
    "# # This dataset also has a duplicate column, dropping...\n",
    "# merged_df.drop(['Fwd Header Length.1'], axis=1, inplace=True)\n",
    "\n",
    "# merged_df = merged_df.sample(frac=1, random_state=29)\n",
    "    \n",
    "# merged_df.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Flow ID',\n",
       " 'Source IP',\n",
       " 'Source Port',\n",
       " 'Destination IP',\n",
       " 'Destination Port',\n",
       " 'Protocol',\n",
       " 'Timestamp',\n",
       " 'Flow Duration',\n",
       " 'Total Fwd Packets',\n",
       " 'Total Backward Packets',\n",
       " 'Total Length of Fwd Packets',\n",
       " 'Total Length of Bwd Packets',\n",
       " 'Fwd Packet Length Max',\n",
       " 'Fwd Packet Length Min',\n",
       " 'Fwd Packet Length Mean',\n",
       " 'Fwd Packet Length Std',\n",
       " 'Bwd Packet Length Max',\n",
       " 'Bwd Packet Length Min',\n",
       " 'Bwd Packet Length Mean',\n",
       " 'Bwd Packet Length Std',\n",
       " 'Flow Bytes/s',\n",
       " 'Flow Packets/s',\n",
       " 'Flow IAT Mean',\n",
       " 'Flow IAT Std',\n",
       " 'Flow IAT Max',\n",
       " 'Flow IAT Min',\n",
       " 'Fwd IAT Total',\n",
       " 'Fwd IAT Mean',\n",
       " 'Fwd IAT Std',\n",
       " 'Fwd IAT Max',\n",
       " 'Fwd IAT Min',\n",
       " 'Bwd IAT Total',\n",
       " 'Bwd IAT Mean',\n",
       " 'Bwd IAT Std',\n",
       " 'Bwd IAT Max',\n",
       " 'Bwd IAT Min',\n",
       " 'Fwd PSH Flags',\n",
       " 'Bwd PSH Flags',\n",
       " 'Fwd URG Flags',\n",
       " 'Bwd URG Flags',\n",
       " 'Fwd Header Length',\n",
       " 'Bwd Header Length',\n",
       " 'Fwd Packets/s',\n",
       " 'Bwd Packets/s',\n",
       " 'Min Packet Length',\n",
       " 'Max Packet Length',\n",
       " 'Packet Length Mean',\n",
       " 'Packet Length Std',\n",
       " 'Packet Length Variance',\n",
       " 'FIN Flag Count',\n",
       " 'SYN Flag Count',\n",
       " 'RST Flag Count',\n",
       " 'PSH Flag Count',\n",
       " 'ACK Flag Count',\n",
       " 'URG Flag Count',\n",
       " 'CWE Flag Count',\n",
       " 'ECE Flag Count',\n",
       " 'Down/Up Ratio',\n",
       " 'Average Packet Size',\n",
       " 'Avg Fwd Segment Size',\n",
       " 'Avg Bwd Segment Size',\n",
       " 'Fwd Avg Bytes/Bulk',\n",
       " 'Fwd Avg Packets/Bulk',\n",
       " 'Fwd Avg Bulk Rate',\n",
       " 'Bwd Avg Bytes/Bulk',\n",
       " 'Bwd Avg Packets/Bulk',\n",
       " 'Bwd Avg Bulk Rate',\n",
       " 'Subflow Fwd Packets',\n",
       " 'Subflow Fwd Bytes',\n",
       " 'Subflow Bwd Packets',\n",
       " 'Subflow Bwd Bytes',\n",
       " 'Init_Win_bytes_forward',\n",
       " 'Init_Win_bytes_backward',\n",
       " 'act_data_pkt_fwd',\n",
       " 'min_seg_size_forward',\n",
       " 'Active Mean',\n",
       " 'Active Std',\n",
       " 'Active Max',\n",
       " 'Active Min',\n",
       " 'Idle Mean',\n",
       " 'Idle Std',\n",
       " 'Idle Max',\n",
       " 'Idle Min',\n",
       " 'Label']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the df into Train and Test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_size = int(0.75 * len(merged_df))\n",
    "combined_train_df = merged_df[:slice_size].copy(deep=True)\n",
    "combined_test_df = merged_df[slice_size:].copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250409_050526\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.8\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.3.0: Thu Jan  2 20:22:58 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8132\n",
      "CPU Count:          10\n",
      "Memory Avail:       3.01 GB / 16.00 GB (18.8%)\n",
      "Disk Space Avail:   280.16 GB / 460.43 GB (60.8%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"/Users/navya/Documents/Uni/Repos/autogluon/AutogluonModels/ag-20250409_050526\"\n",
      "Train Data Rows:    4000\n",
      "Train Data Columns: 83\n",
      "Label Column:       Label\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\t9 unique label values:  ['DoS Hulk', 'PortScan', 'DDoS', 'FTP-Patator', 'DoS GoldenEye', 'DoS Slowhttptest', 'DoS slowloris', 'SSH-Patator', 'BENIGN']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 9\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3098.73 MB\n",
      "\tTrain Data (Original)  Memory Usage: 3.56 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 12): ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'RST Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['SYN Flag Count', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 5 | ['SYN Flag Count', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 45 | ['Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', ...]\n",
      "\t\t('int', [])                        : 17 | ['Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', ...]\n",
      "\t\t('object', [])                     :  3 | ['Flow ID', 'Source IP', 'Destination IP']\n",
      "\t\t('object', ['datetime_as_object']) :  1 | ['Timestamp']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             :  3 | ['Flow ID', 'Source IP', 'Destination IP']\n",
      "\t\t('float', [])                : 45 | ['Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', ...]\n",
      "\t\t('int', [])                  : 12 | ['Source Port', 'Destination Port', 'Protocol', 'Flow Duration', 'Total Fwd Packets', ...]\n",
      "\t\t('int', ['bool'])            :  5 | ['Fwd PSH Flags', 'FIN Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count']\n",
      "\t\t('int', ['datetime_as_int']) :  3 | ['Timestamp', 'Timestamp.month', 'Timestamp.dayofweek']\n",
      "\t0.4s = Fit runtime\n",
      "\t66 features in original data used to generate 68 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.44s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.125, Train Rows: 3500, Val Rows: 500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.56s of the 59.56s of remaining time.\n",
      "\t0.8786\t = Validation score   (mcc)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 58.71s of the 58.71s of remaining time.\n",
      "\t0.8824\t = Validation score   (mcc)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 58.70s of the 58.69s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.9972\t = Validation score   (mcc)\n",
      "\t2.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 56.42s of the 56.41s of remaining time.\n",
      "\t0.9972\t = Validation score   (mcc)\n",
      "\t6.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 49.92s of the 49.91s of remaining time.\n",
      "\t1.0\t = Validation score   (mcc)\n",
      "\t6.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 43.28s of the 43.28s of remaining time.\n",
      "\t1.0\t = Validation score   (mcc)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 42.94s of the 42.93s of remaining time.\n",
      "\t1.0\t = Validation score   (mcc)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 42.60s of the 42.60s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 1499.\n",
      "\t1.0\t = Validation score   (mcc)\n",
      "\t42.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 60.01s ... Best model: LightGBM | Estimated inference throughput: 264691.7 rows/s (500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/navya/Documents/Uni/Repos/autogluon/AutogluonModels/ag-20250409_050526\")\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "all_results = []\n",
    "# for train_data_size in range(2000, 6000, 2000):\n",
    "train_data_size = 2000\n",
    "combined_train_df_slice = pd.DataFrame()\n",
    "for malicious_label_name in list(malicious_df['Label'].unique()):\n",
    "    malicious_slice = malicious_df.loc[malicious_df['Label'] == malicious_label_name][:int(train_data_size/8)]\n",
    "    combined_train_df_slice = pd.concat([combined_train_df_slice, malicious_slice])\n",
    "combined_train_df_slice = pd.concat([combined_train_df_slice, benign_df[:train_data_size]])\n",
    "\n",
    "print(len(combined_train_df_slice))\n",
    "\n",
    "predictor = TabularPredictor(label='Label', eval_metric='mcc').fit(combined_train_df_slice, time_limit=60, presets='medium_quality', fit_weighted_ensemble=False)\n",
    "\n",
    "y_pred = predictor.predict(combined_test_df.drop(columns=['Label']))\n",
    "all_results.append(predictor.evaluate(combined_test_df, silent=True))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mcc': 0.9938331690493879,\n",
       "  'accuracy': 0.9979006238631921,\n",
       "  'balanced_accuracy': 0.9951110602012148}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Export Non Persistent Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloned TabularPredictor located in '/Users/navya/Documents/Uni/Repos/autogluon/AutogluonModels/ag-20250409_050526' to 'non-persistent-multiclass-classifier'.\n",
      "\tTo load the cloned predictor: predictor_clone = TabularPredictor.load(path=\"non-persistent-multiclass-classifier\")\n",
      "Clone: Keeping minimum set of models required to predict with best model 'LightGBM'...\n",
      "Deleting model KNeighborsUnif. All files under /Users/navya/Documents/Uni/Repos/autogluon/non-persistent-multiclass-classifier/models/KNeighborsUnif will be removed.\n",
      "Deleting model KNeighborsDist. All files under /Users/navya/Documents/Uni/Repos/autogluon/non-persistent-multiclass-classifier/models/KNeighborsDist will be removed.\n",
      "Deleting model NeuralNetFastAI. All files under /Users/navya/Documents/Uni/Repos/autogluon/non-persistent-multiclass-classifier/models/NeuralNetFastAI will be removed.\n",
      "Deleting model LightGBMXT. All files under /Users/navya/Documents/Uni/Repos/autogluon/non-persistent-multiclass-classifier/models/LightGBMXT will be removed.\n",
      "Deleting model RandomForestGini. All files under /Users/navya/Documents/Uni/Repos/autogluon/non-persistent-multiclass-classifier/models/RandomForestGini will be removed.\n",
      "Deleting model RandomForestEntr. All files under /Users/navya/Documents/Uni/Repos/autogluon/non-persistent-multiclass-classifier/models/RandomForestEntr will be removed.\n",
      "Deleting model CatBoost. All files under /Users/navya/Documents/Uni/Repos/autogluon/non-persistent-multiclass-classifier/models/CatBoost will be removed.\n",
      "Clone: Removing artifacts unnecessary for prediction. NOTE: Clone can no longer fit new models, and most functionality except for predict and predict_proba will no longer work\n"
     ]
    }
   ],
   "source": [
    "save_path_clone_opt = \"non-persistent-multiclass-classifier\"\n",
    "# will return the path to the cloned predictor, identical to save_path_clone_opt\n",
    "path_clone_opt = predictor.clone_for_deployment(path=save_path_clone_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
